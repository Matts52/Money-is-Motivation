---
title: "Term_Project_Class_Tree"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---
title: "Term_project"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(root.dir = "C:/Users/senic/OneDrive/Desktop/Masters/FALL_2021/ECO1400/Term_Paper/Data")
```

```{r}
chooseCRANmirror(graphics=FALSE, ind=62)
if (!require("rsample")) install.packages("rsample") 
if (!require("dplyr")) install.packages("dplyr") 
if (!require("rpart")) install.packages("rpart") 
if (!require("rpart.plot")) install.packages("rpart.plot") 
if (!require("ipred")) install.packages("ipred") 
if (!require("randomForest")) install.packages("randomForest")
if (!require("ranger")) install.packages("ranger") 
if (!require("caret")) install.packages("caret") 
if (!require("MASS")) install.packages("MASS") 
if (!require("e1071")) install.packages("e1071")
if (!require("ROCR")) install.packages("ROCR")

library(rsample)      # data splitting, functions: initial_split 
library(dplyr)        # data wrangling
library(rpart)        # performing regression trees
library(rpart.plot)   # plotting regression trees
library(ipred)        # bagging
library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest
library(caret)        # an aggregator package for performing many machine learning models
library(MASS)         # dataset
library(e1071)
library(ROCR)

```

Load in the data
```{r}
setwd("C:/Users/senic/OneDrive/Desktop/Masters/FALL_2021/ECO1400/Term_Paper/Data")
dataset_full <- read.csv(file="All_Disper_1_2_3_2011_2018.csv", header=TRUE, sep=",", na = ".", colClasses = c("HomeWinBinary"="factor"))
```

```{r}
dataset_full <- dataset_full[dataset_full$HomeAveSal != 0, ]
dataset_full <- dataset_full[dataset_full$HomeAveSal != 0, ]
```

Add delta columns to the existing dataframe
```{r}

dataset_full$deltaScore = dataset_full$HomeScore/dataset_full$AwayScore
dataset_full$deltaMaxSal = dataset_full$HomeMaxSal/dataset_full$AwayMaxSal
dataset_full$deltaAveSal = dataset_full$HomeAveSal/dataset_full$AwayAveSal
dataset_full$deltaDisper1 = dataset_full$HomeDisper1/dataset_full$AwayDisper1
dataset_full$deltaDisper2 = dataset_full$HomeDisper2/dataset_full$AwayDisper2
dataset_full$deltaDisper3 = dataset_full$HomeDisper3/dataset_full$AwayDisper3

```

Create training (70%) and test (30%) sets for the datasetHousing::make_dataset() data.
Use set.seed for reproducibility
```{r}
#na.omit(dataset_full)

set.seed(123)
dataset_split <- initial_split(na.omit(dataset_full), prop = 0.7)
dataset_train <- training(na.omit(dataset_split))
dataset_test  <- testing(na.omit(dataset_split))
```

Fit a regression tree
```{r}
m1 <- rpart(formula = HomeWinBinary ~ HomeMaxSal + AwayMaxSal + HomeAveSal + AwayAveSal + HomeDisper1 + AwayDisper1 + HomeDisper2 + AwayDisper2 + HomeDisper3 + AwayDisper3, data = dataset_train, method = "class")
```


Visualize the fitted tree 
```{r}
rpart.plot(m1, cex=0.5)
```



*************************** RANDOM FORESTS ***************************

Random forests are a modification of bagging that builds a large collection of de-correlated trees, by inducing more randomness into the tree growing process. This is achieved in two stages:

1. Bootstrap: similar to bagging, where each tree is grown to a bootstrap resampled data set.
2. Split-variable randomization: each time a split is to be performed, the search for the split variable is limited to a random subset of m of the p variables. 

Fit a random forest model
```{r}
rf <- randomForest(formula = HomeWinBinary ~ HomeMaxSal + AwayMaxSal + HomeAveSal + AwayAveSal + HomeDisper1 + AwayDisper1 + HomeDisper2 + AwayDisper2 + HomeDisper3 + AwayDisper3, data = dataset_train, ntrees = 100, na.action=na.omit, importance=TRUE, mtry=5)
```


HomeMaxSal + AwayMaxSal + HomeAveSal + AwayAveSal + HomeDisper1 + AwayDisper1 + HomeDisper2 + AwayDisper2 + HomeDisper3 + AwayDisper3

deltaAveSal + deltaMaxSal + deltaDisper1 + deltaDisper2 + deltaDisper3


Display the model
```{r}
rf

```

Plot the error rate
```{r, fig.width=4, fig.height=3}
plot(rf)
```




```{r}
varImpPlot(rf)
```

Check predictive accuracy on new data
```{r}
pred_randomForest <- predict(rf, newdata=dataset_test)
table(factor(pred_randomForest, levels=0:1), factor(dataset_test[,19], levels=0:1))

```



Plot the ROC curve (pink is for 0 (loss), green is for 1 (win))
```{r}

pred_roc <- predict(rf, newdata=dataset_test, type="prob")
pretty_colours <- c("#F8766D","#00BA38")
classes <- levels(dataset_test$HomeWinBinary)

for (i in 1:2)
{
 # Define which observations belong to class[i]
 true_values <- ifelse(dataset_test[,19]==classes[i],1,0)
 # Assess the performance of classifier for class[i]
 pred <- prediction(pred_roc[,i],true_values)
 perf <- performance(pred, "tpr", "fpr")
 if (i==1)
 {
     plot(perf,main="ROC Curve",col=pretty_colours[i]) 
 }
 else
 {
     plot(perf,main="ROC Curve",col=pretty_colours[i],add=TRUE) 
 }
 # Calculate the AUC and print it to screen
 auc.perf <- performance(pred, measure = "auc")
 print(auc.perf@y.values)
}


```

